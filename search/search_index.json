{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Index \u00b6 This documentation discusses the paper Nader N. Nashed, Seif Eldawlatly and Gamal M Aly, \"A deep learning approach to single-trial classification for P300 spellers\", IEEE Middle East Conference on Biomedical Engineering, vol. 2018, no. 1, pp. 11-16, July 2018. Documentation Map \u00b6 Introduction Methodology Results References","title":"Index"},{"location":"#index","text":"This documentation discusses the paper Nader N. Nashed, Seif Eldawlatly and Gamal M Aly, \"A deep learning approach to single-trial classification for P300 spellers\", IEEE Middle East Conference on Biomedical Engineering, vol. 2018, no. 1, pp. 11-16, July 2018.","title":"Index"},{"location":"#documentation-map","text":"Introduction Methodology Results References","title":"Documentation Map"},{"location":"introduction/","text":"Introduction \u00b6 Goal of the paper The goal of the paper is to investigate the deep learning approach in achieving a higher accuracy while communicating the desired characters with the P300 speller from as little as a single trial. Brain Computer Interface (BCI) technology could enable communication with individuals with server motor disabilities, by communicating directly with the brain signals. To achieve BCI, one needs to determine: How the electrods are to be placed Which brain activity to be considered Which computer program to use Which learning technique to use In this introduction we clarify the context that this paper working on. Electrode Placement \u00b6 To make a BCI, one needs to place electrodes onto the individual's head, the are 3 ways to achieve such placement: non-invasive, semi-invasive and invasive, the following figure and table illustrate that further: Different BCI electrode placements [1] Method Non-Invasive Semi-Invasive Non-Invasive Recorded brain activity Electroencephalographic (EEG) Electrocorticography (ECoG) Individual neurons Electrode placement Over the head Over the brain Inside the brain Surgical operation Not required Required Required Spatial Resolution Least Medium High Paper Approach This paper investigates the usage of EEG based activities. P300 Speller \u00b6 Time diagram of a P300 speller [10] The computer program used for the interface is a P300 speller which works by showing the user a MxN matrix of the characters of interest (6x6 matrix for this paper) and flashing each row and column for K times (called K trials), for example if K is 1 (single trial) there will be 12 flashes (6 different row flashes and 6 different column flashes), K is 5 (5 trials) there will be 60 flashes (30 row flashes and 30 column flashes). The flashes occur randomly and have Interintisification Intervals (ISI) of 100 ms between them. The user needs to focus on the character he wants to type each time, and once the screen flashes in front of him a brain activity shall evoke after 300 milliseconds which could be detected to figure out that the character the user is looking at is in the flashed row or column, 300 ms ago. Increasing the number of trials, would increase the detection accuracy, however each extra trial involves extra 12 flashes. Paper Approach This paper utilizes the reads P300 brain activitiy, and uses the P300 speller as the software for interaction. It also observes the effect of the deep learning approach on the decrease of the trials. Learning Techniques \u00b6 Artificial Neural Network [14] To learn from the brain activities both feature extraction and classification shall be utilized. There are multiple techniques for doing so. The classification problem in hand is considered a supervised learning one, as the classes are pre-known {P300 and non-P300} . Different approaches could be considered to learn the model, for example Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), Independent Componant Analysis (ICA), autoencoders, Fisher's Linear Discriminant Classifier, Maximum Likelihood Discriminant Classifier KNN, etc. Paper Approach This paper addresses the deep learning technique for the model, namely using stacked autoencoders with softmax a differentiable version of argmax , to learn the data compared to the usage of Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) in previous models.","title":"Introduction"},{"location":"introduction/#introduction","text":"Goal of the paper The goal of the paper is to investigate the deep learning approach in achieving a higher accuracy while communicating the desired characters with the P300 speller from as little as a single trial. Brain Computer Interface (BCI) technology could enable communication with individuals with server motor disabilities, by communicating directly with the brain signals. To achieve BCI, one needs to determine: How the electrods are to be placed Which brain activity to be considered Which computer program to use Which learning technique to use In this introduction we clarify the context that this paper working on.","title":"Introduction"},{"location":"introduction/#electrode-placement","text":"To make a BCI, one needs to place electrodes onto the individual's head, the are 3 ways to achieve such placement: non-invasive, semi-invasive and invasive, the following figure and table illustrate that further: Different BCI electrode placements [1] Method Non-Invasive Semi-Invasive Non-Invasive Recorded brain activity Electroencephalographic (EEG) Electrocorticography (ECoG) Individual neurons Electrode placement Over the head Over the brain Inside the brain Surgical operation Not required Required Required Spatial Resolution Least Medium High Paper Approach This paper investigates the usage of EEG based activities.","title":"Electrode Placement"},{"location":"introduction/#p300-speller","text":"Time diagram of a P300 speller [10] The computer program used for the interface is a P300 speller which works by showing the user a MxN matrix of the characters of interest (6x6 matrix for this paper) and flashing each row and column for K times (called K trials), for example if K is 1 (single trial) there will be 12 flashes (6 different row flashes and 6 different column flashes), K is 5 (5 trials) there will be 60 flashes (30 row flashes and 30 column flashes). The flashes occur randomly and have Interintisification Intervals (ISI) of 100 ms between them. The user needs to focus on the character he wants to type each time, and once the screen flashes in front of him a brain activity shall evoke after 300 milliseconds which could be detected to figure out that the character the user is looking at is in the flashed row or column, 300 ms ago. Increasing the number of trials, would increase the detection accuracy, however each extra trial involves extra 12 flashes. Paper Approach This paper utilizes the reads P300 brain activitiy, and uses the P300 speller as the software for interaction. It also observes the effect of the deep learning approach on the decrease of the trials.","title":"P300 Speller"},{"location":"introduction/#learning-techniques","text":"Artificial Neural Network [14] To learn from the brain activities both feature extraction and classification shall be utilized. There are multiple techniques for doing so. The classification problem in hand is considered a supervised learning one, as the classes are pre-known {P300 and non-P300} . Different approaches could be considered to learn the model, for example Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), Independent Componant Analysis (ICA), autoencoders, Fisher's Linear Discriminant Classifier, Maximum Likelihood Discriminant Classifier KNN, etc. Paper Approach This paper addresses the deep learning technique for the model, namely using stacked autoencoders with softmax a differentiable version of argmax , to learn the data compared to the usage of Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) in previous models.","title":"Learning Techniques"},{"location":"methodology/","text":"Methodology \u00b6 The methodology followed in the paper follows this Diagram Block Diagram [13] Data Acquisation \u00b6 Data collected from an Emotiv Epoc which is installed based on the 10-20 international system. That is digitized and sampled at 240Hz . and bandpass filtered in the range of 0.4Hz to 45Hz . International 10-20 system [5] The collected data consists of: Data from 2 healthy individuals. 36 character training data. 32 character test data. Each epoch of the dataset included data from: 14 electrodes 12 intensifications of 100ms interval each (6 rows and 6 columns) repeated for 15 trials. Data Preprocessing \u00b6 Following are the data preprocessing steps taken in order over the data Common Average Reference (CAR) is executed first by removing the average of all the electrode from each electrode. Moving Average Filter is run over a 25-sample window. Z-Scores of each recorded data is computed to normalize the data using the mean and standard deviation. Feature Extraction \u00b6 Features were extracted from only 4 electrodes (responsible for the visual processing {O1, O2} and cognitive processing {P7, P8} ) for this paper's approach, and then compared to the 14 electrodes approach as found in the Results section Previous Approach \u00b6 The Principal Component Analysis (PCA) was used in reducing the dimensionality for the input data to extract the necessary features in an unsupervised manner. Paper Approach \u00b6 3 layers of stacked sparse autoencoders are used. The autoencoder which allows non-linear transformation. over the linear transformation available through the PCA. Multiple activation functions were investigated for this approach. Autoencoders \u00b6 Stacked autoencoders [13] An autoencoder is a non-linear classifier which is powerful in capturing the minimum number of features required to classify the data. It has the following properties: Consists of hidden units Learns the hidden unit parameters by trying to reconstruct the input into the output neurons. Requires correct choice of number of features. Non-linear, which could make it easily overfitting in comparison with other linear techniques like the PCA. Which requires good selection of the input data and features. PCA vs autoencoder [3] Classification \u00b6 The problem in hand involves a supervised classification of the targets {P300, non P300}. Previous Approach \u00b6 The supervised Linear Discriminant Analysis (LDA) was used in the previous approach. This is a classifier that uses the data labels to classify by projecting the data on the directions of: Maximum variance between classes. Minimum variance within classes. Paper Approach \u00b6 The softmax regresion classifier is used for this approach. Which has the following properties: A generic version of logistic regression Number of outputs same as number of inputs Sum of the output neuron values is 1 Differential which eases backprobagation Unless similar output node values exist One of the outputs would be close to 1, the others would be almost 0 Softmax computation example [8]","title":"Methodology"},{"location":"methodology/#methodology","text":"The methodology followed in the paper follows this Diagram Block Diagram [13]","title":"Methodology"},{"location":"methodology/#data-acquisation","text":"Data collected from an Emotiv Epoc which is installed based on the 10-20 international system. That is digitized and sampled at 240Hz . and bandpass filtered in the range of 0.4Hz to 45Hz . International 10-20 system [5] The collected data consists of: Data from 2 healthy individuals. 36 character training data. 32 character test data. Each epoch of the dataset included data from: 14 electrodes 12 intensifications of 100ms interval each (6 rows and 6 columns) repeated for 15 trials.","title":"Data Acquisation"},{"location":"methodology/#data-preprocessing","text":"Following are the data preprocessing steps taken in order over the data Common Average Reference (CAR) is executed first by removing the average of all the electrode from each electrode. Moving Average Filter is run over a 25-sample window. Z-Scores of each recorded data is computed to normalize the data using the mean and standard deviation.","title":"Data Preprocessing"},{"location":"methodology/#feature-extraction","text":"Features were extracted from only 4 electrodes (responsible for the visual processing {O1, O2} and cognitive processing {P7, P8} ) for this paper's approach, and then compared to the 14 electrodes approach as found in the Results section","title":"Feature Extraction"},{"location":"methodology/#previous-approach","text":"The Principal Component Analysis (PCA) was used in reducing the dimensionality for the input data to extract the necessary features in an unsupervised manner.","title":"Previous Approach"},{"location":"methodology/#paper-approach","text":"3 layers of stacked sparse autoencoders are used. The autoencoder which allows non-linear transformation. over the linear transformation available through the PCA. Multiple activation functions were investigated for this approach.","title":"Paper Approach"},{"location":"methodology/#autoencoders","text":"Stacked autoencoders [13] An autoencoder is a non-linear classifier which is powerful in capturing the minimum number of features required to classify the data. It has the following properties: Consists of hidden units Learns the hidden unit parameters by trying to reconstruct the input into the output neurons. Requires correct choice of number of features. Non-linear, which could make it easily overfitting in comparison with other linear techniques like the PCA. Which requires good selection of the input data and features. PCA vs autoencoder [3]","title":"Autoencoders"},{"location":"methodology/#classification","text":"The problem in hand involves a supervised classification of the targets {P300, non P300}.","title":"Classification"},{"location":"methodology/#previous-approach_1","text":"The supervised Linear Discriminant Analysis (LDA) was used in the previous approach. This is a classifier that uses the data labels to classify by projecting the data on the directions of: Maximum variance between classes. Minimum variance within classes.","title":"Previous Approach"},{"location":"methodology/#paper-approach_1","text":"The softmax regresion classifier is used for this approach. Which has the following properties: A generic version of logistic regression Number of outputs same as number of inputs Sum of the output neuron values is 1 Differential which eases backprobagation Unless similar output node values exist One of the outputs would be close to 1, the others would be almost 0 Softmax computation example [8]","title":"Paper Approach"},{"location":"references/","text":"References \u00b6 Following are references either used in this documentation or include suplementary materials for this documentation and could aid in having broader understanding of different topics: [1]: bmseed: electrode types [2]: Rami Khushaba: PCA vs LDA [3]: Valerio Velardo: Autoencoders Explained Easily [4]: Interacoustics: P300 Evoked Potential: An Introduction Part 1 and Part 2 [5]: Wikipedia: 10-20 system (EEG) [6]: Stanford University - Andrew NG: Sparse Autoencoder [7]: CS231n Convolutional Neural Networks for Visual Recognition [8]: DeepAI: Softmax Classifier [9]: Stanford University: Sparse Autoencoder [10]: Research Gate: Stimulus and operating principle of P300-speller. When the subject counts the number of flashes of the character \"K\", P300 is elicited by the user's response. [11]: Wikipedia: Autoencoder [12]: Braingizer: A Brain-Controlled Wheelchair [13]: IEEE Middle East Conference on Biomedical Engineering: Nader N. Nashed, Seif Eldawlatly and Gamal M Aly: A deep learning approach to single-trial classification for P300 spellers [14]: Wikipedia: Artificial Neural Network","title":"References"},{"location":"references/#references","text":"Following are references either used in this documentation or include suplementary materials for this documentation and could aid in having broader understanding of different topics: [1]: bmseed: electrode types [2]: Rami Khushaba: PCA vs LDA [3]: Valerio Velardo: Autoencoders Explained Easily [4]: Interacoustics: P300 Evoked Potential: An Introduction Part 1 and Part 2 [5]: Wikipedia: 10-20 system (EEG) [6]: Stanford University - Andrew NG: Sparse Autoencoder [7]: CS231n Convolutional Neural Networks for Visual Recognition [8]: DeepAI: Softmax Classifier [9]: Stanford University: Sparse Autoencoder [10]: Research Gate: Stimulus and operating principle of P300-speller. When the subject counts the number of flashes of the character \"K\", P300 is elicited by the user's response. [11]: Wikipedia: Autoencoder [12]: Braingizer: A Brain-Controlled Wheelchair [13]: IEEE Middle East Conference on Biomedical Engineering: Nader N. Nashed, Seif Eldawlatly and Gamal M Aly: A deep learning approach to single-trial classification for P300 spellers [14]: Wikipedia: Artificial Neural Network","title":"References"},{"location":"results/","text":"Results \u00b6 Single Trial Signal \u00b6 By reducing the number of trials to one, the accuracy of the detected signal drops significantly which imposes a challenge in correctly classifying from a single trial Number of Trials \u00b6 By varying the number of trials we find that even when PCA-based approach had an increase of ~2% when the number of trials was high (15 trials), the approach in this paper excels by ~8% in the single trial, and ~5% for 5 trials Number of channels in classification \u00b6 Having more channels (14 channels) in comparison with the 4 selected channels for classification in the proposed approach resulted in a decrease in the results by ~13% , even when the increase of channels resulted in an increase of ~7% Activation Function \u00b6 Linear activation exceled over the Logistic Sigmoid activation (by ~16% ) and over the Positive Saturating Linear function (by ~11% ) Hidden Layer Size of Stacked Autoencoders \u00b6 Multiple sizes of the autoencoders in the hidden layers were used, and the highest accuracy of them are the one that gradually decreases in number over layers, until it reaches the bottleneck, showing enhancements up to 11% Conclusion \u00b6 Results obtained from introducing a stacked autoencoder deep learning technique for P300 speller show a noticeable increase in classifying single trials over the PCA-based methods. The work could be extended by usage of: Convolutional Neural Networks could be considered. Variational autoencoders and comparing its performance with the different stack size selections.","title":"Results"},{"location":"results/#results","text":"","title":"Results"},{"location":"results/#single-trial-signal","text":"By reducing the number of trials to one, the accuracy of the detected signal drops significantly which imposes a challenge in correctly classifying from a single trial","title":"Single Trial Signal"},{"location":"results/#number-of-trials","text":"By varying the number of trials we find that even when PCA-based approach had an increase of ~2% when the number of trials was high (15 trials), the approach in this paper excels by ~8% in the single trial, and ~5% for 5 trials","title":"Number of Trials"},{"location":"results/#number-of-channels-in-classification","text":"Having more channels (14 channels) in comparison with the 4 selected channels for classification in the proposed approach resulted in a decrease in the results by ~13% , even when the increase of channels resulted in an increase of ~7%","title":"Number of channels in classification"},{"location":"results/#activation-function","text":"Linear activation exceled over the Logistic Sigmoid activation (by ~16% ) and over the Positive Saturating Linear function (by ~11% )","title":"Activation Function"},{"location":"results/#hidden-layer-size-of-stacked-autoencoders","text":"Multiple sizes of the autoencoders in the hidden layers were used, and the highest accuracy of them are the one that gradually decreases in number over layers, until it reaches the bottleneck, showing enhancements up to 11%","title":"Hidden Layer Size of Stacked Autoencoders"},{"location":"results/#conclusion","text":"Results obtained from introducing a stacked autoencoder deep learning technique for P300 speller show a noticeable increase in classifying single trials over the PCA-based methods. The work could be extended by usage of: Convolutional Neural Networks could be considered. Variational autoencoders and comparing its performance with the different stack size selections.","title":"Conclusion"}]}